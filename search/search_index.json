{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Collaborative Robot Assistant for Object Transport &amp; Assembly","text":""},{"location":"#team-information","title":"Team Information","text":"<p>Team Number: 6  Team Members: Jeremy Chen, Chieh Kao, Hsing-Cheng Wang  Semester and Year: Spring 2025  School: Arizona State University  Class: RAS598 - Experimentation and Deployment of Robotic Systems  Professor: Daniel Aukes </p>"},{"location":"#introduction","title":"Introduction","text":"<p>As a variety of robots are being increasingly used in manufacturing settings, we are inspired to create a minimal project that covers the three aspects of automated manufacturing: application, collaboration, and simulation. In a real-world warehouse, mobile robots transport items, while robotic arms handle detailed tasks like picking &amp; placing, sorting, and assembly. In this project, we will leverage ROS 2 for communication and use sensor data for autonomous navigation with the TurtleBot. We will be integrating a TurtleBot 4 Lite and CV2 package  for object transport and manipulation. We would also like to dig into the concept of creating digital twins to simulate the behavior of each robot and further verify it with a real-world scenario.</p> <p>Research Question \"A minimal automated manufacturing setting including an object transporting robot, object manipulating robot arm, and a real-time simulation.\" </p> <p>Sample Figure The following image is created by AI just for explanation purposes. </p>"},{"location":"#sensor-integration","title":"Sensor Integration","text":""},{"location":"#how-will-sensor-data-be-used-in-the-code","title":"How will sensor data be used in the code?","text":"<ul> <li>TurtleBot 4 Sensors:<ul> <li>LIDAR &amp; Depth Camera: Used for object detection, localization, and obstacle avoidance.</li> <li>IMU (Inertial Measurement Unit): Helps improve localization and detect unexpected collisions.</li> <li>Wheel rotation tracker: For the localization of turtlebot.</li> </ul> </li> </ul> <p>To utilize sensors on the TurtleBot, we plan to feed the sensor inputs, including the CV2 package object detection system, to our local host computer because we suspect that SLAM navigation will require significant computing power, which might overwhelm the Raspberry Pi on the TurtleBot. By inputting the LIDAR and depth camera sensor data, along with the CV2 package-based traffic light detection, to the local computing power, we will return the navigation results to the TurtleBot to command its movement.</p>"},{"location":"#interaction-user-interface","title":"Interaction &amp; User Interface","text":"<p>To control and monitor the robots, we will be building a dashboard on the ROS-based UI. The dashboard will include to separate sections as shown in the figure below. On the left side will be the simulation, and on the right side will be all the controls needed. The controls will simply be start, stop, and emergency stop. Another window will show the path that is rendered from the data from Turtlebot.   </p> <p> GUI(05/01/2025)</p>"},{"location":"#control-autonomy","title":"Control &amp; Autonomy","text":"<p>On the TurtleBot, we will use sensor-based autonomous navigation (e.g., LiDAR and depth camera data for obstacle avoidance and path planning). The TurtleBot will detect when it has reached its destination, and upon doing so, it will send a ROS signal\u2014such as detecting a QR code on the wall, at the final result, we us color as the destination.</p> <p>Once the robot arm receives the ROS signal from the TurtleBot, it will move toward the object and adjust its speed according to the traffic light signal. The TurtleBot will continue navigating, and once it reaches a certain distance from the target, the robot arm will perform the necessary manipulation. Finally, the system will send a completion signal to the TurtleBot, enabling it to proceed to the next task.</p>"},{"location":"#preparation-needs","title":"Preparation Needs","text":"<p>In order to build a Collaborative Robot Assistant with the TurtleBot 4, we need to understand ROS 2 communication, including Topics for real-time data exchange, Services for request-response interactions, and Actions for handling asynchronous tasks.Sensor-based navigation techniques will be applied to ensure the TurtleBot\u2019s accurate movement and positioning, ensuring robust mapping and localization. Additionally, we will use Gazebo simulation for testing and refining the system in a virtual environment.</p>"},{"location":"#final-demonstration","title":"Final Demonstration","text":"<p>In this demonstration, we will showcase a minimal automated manufacturing setup in which a TurtleBot 4 autonomously navigates while transporting an object.</p> <p>The process begins with the TurtleBot undocking and detecting its target. Upon arrival at the target location, the system will trigger a request for the item to be placed onto the TurtleBot. Once the item is successfully loaded onto the TurtleBot, it will move to the designated unloading area.</p> <p>After reaching the unloading area, the TurtleBot will deposit the item and return to its original position, completing the task.</p> <p>During this movement, the TurtleBot will utilize the CV2 package to detect traffic lights in its environment and interpret the signal color. The decision-making process based on traffic light detection is as follows:</p> <p>Red light: The TurtleBot will continue moving forward until it close enough to the red object(depend by pixels). Green light: The TurtleBot will continue moving forward at its normal speed. Blue light: The TurtleBot will slow down and proceed with caution.</p> <p>In the event of conflicting signals, the following prioritization will apply:</p> <p>If both green and blue lights are detected, the blue light will take priority. If red, blue, and green lights are detected simultaneously, the red light will be prioritized, causing the TurtleBot go toward to it and stop. When the turtlebot reached the red destination, it will send a signal to UR5 robot arm, then UR5 robot will go pick the object from turtlebot then place it to the processing table.</p> <p>This decision-making process ensures that the TurtleBot behaves appropriately in various traffic scenarios. The TurtleBot will continue to follow its designated path, tracking its target according to the programmed instructions. If it encounters a traffic light, it will adjust its movement based on the detected signal color, ensuring safe and efficient navigation in a simulated traffic environment.</p> <p>Instead of SLAM, we utilize pre-defined waypoints and real-time sensor feedback for navigation to simplify processing and improve responsiveness on the TurtleBot. The system will demonstrate real-time multi-robot collaboration, SLAM-based navigation, and adaptive object manipulation, simulating an industrial setting where autonomous robots interact seamlessly to perform tasks.</p>"},{"location":"#resources-needed","title":"Resources Needed","text":"<ul> <li>TurtleBot 4 with a carrying tray(maybe)</li> <li>TurtleBot 4 (possibly equipped with a carrying tray for object transport)</li> <li>ROS 2-based computing setup for communication and coordination</li> <li>LiDAR, depth camera, IMU, and force-torque sensors for navigation and manipulation feedback</li> <li>UR5 robot arm</li> </ul>"},{"location":"#classroom-setup","title":"Classroom Setup","text":"<ul> <li>Open floor space: Ensuring sufficient area for TurtleBot 4 to navigate safely.</li> <li>Elevated Track System: Depending on the demonstration needs, we may arrange tables to form a track-like platform for TurtleBot 4, allowing it to operate at the same height as the traffic light system. This will ensure seamless interaction between the TurtleBot and the traffic light system, enabling it to detect and respond to traffic light signals effectively.</li> <li>Minimal environmental interference: Avoid excessive lighting changes or obstructions that may interfere with the LiDAR, depth camera, or CV2 package-based traffic light detection.</li> <li>UR5 robot arm connection needs to be stable thru the Wifi network, and the posture of the robot have to be checked whether it can reach or not.</li> </ul>"},{"location":"#handling-variability","title":"Handling Variability","text":"<p>The system is designed to dynamically adapt to environmental changes by incorporating various strategies. TurtleBot 4 utilizes LiDAR and a depth camera to identify and avoid unexpected obstacles in real-time, ensuring smooth navigation. Depth sensors compensate for lighting variations, allowing the system to function reliably under changing light conditions.</p> <p>TurtleBot will use CV2 package to detect traffic lights in its environment and adjust its speed accordingly. If a green light is detected, it will continue moving at its normal speed. If a yellow light is detected, it will slow down. If a red light is detected, it will stop. This dynamic adjustment ensures that TurtleBot responds appropriately to the traffic signal in real-time.</p> <p>The table track system allows TurtleBot to maintain the required height alignment, ensuring smooth operation and effective interaction with the traffic light system.</p>"},{"location":"#testing-evaluation","title":"Testing &amp; Evaluation","text":"<p>To validate the system\u2019s performance, we will evaluate key metrics that reflect its accuracy, efficiency, and adaptability. Navigation accuracy will be assessed by comparing TurtleBot 4\u2019s planned path with its actual trajectory, measuring deviations and making necessary navigation parameter adjustments. The system\u2019s responsiveness to traffic lights will be evaluated by testing TurtleBot\u2019s ability to detect and react appropriately to traffic light signals (green, blue, and red) in real-time. The time taken to adjust its speed and stop based on the detected signal will also be measured.</p> <p>Task completion time will be used to evaluate the overall efficiency of the TurtleBot in navigating and responding to traffic signals. This will ensure that each task, including navigation and signal response, is completed as swiftly and effectively as possible. Additionally, error recovery will be analyzed by examining the system\u2019s ability to handle failures dynamically, such as detecting traffic lights in challenging lighting conditions or avoiding obstacles that may disrupt the planned path.</p>"},{"location":"#impact","title":"Impact","text":"<p>As ROS2 is a new topic for all of us, we can learn the valuable lesson of how to integrate different systems and also to create a simulation with Gazebo. By going through a hands-on deployment of our system, we can have a grasp on the basic topics including but not limited to: 1. The communication among multiple robots. 2. Creating a simulation with Gazebo. 3. Sensor fusion.</p> <p>We aim to create a reproducible framework for future multi-robot systems.</p>"},{"location":"#advising","title":"Advising","text":"<p>In the near future, we will need guidance from Prof. Aukes, especially on sensor-based autonomous navigation and optimizing TurtleBot 4 Lite\u2019s interaction with its environment. To successfully implement real-time traffic light detection and response, as well as to ensure smooth coordination and data flow, we will need Prof. Aukes\u2019s advice on handling potential barriers in system integration. This will help us ensure that the TurtleBot operates efficiently, detecting and responding to traffic lights, and completes tasks seamlessly in a dynamic environment.</p>"},{"location":"#gantt-chart-project-planning","title":"Gantt Chart Project Planning","text":"<p> Gantt chart(05/01/2025)</p>"},{"location":"#rqt_graph","title":"rqt_graph","text":"<p> rqt_graph(05/01/2025)</p>"},{"location":"#code-breakdown","title":"Code Breakdown","text":"<p>This video shows our code breakdown and all code functions explenation.(05/01/2025)    </p>"},{"location":"#discussion05012025","title":"Discussion(05/01/2025)","text":"<p>In this project, we integrate multiple sensors\u2014camera, IMU, and LiDAR\u2014along with coordinated control of the TurtleBot 4 mobile robot and the UR5 robotic arm to achieve autonomous navigation and interaction with traffic light signals.</p> <p>Camera (YOLO-based Traffic Light Detection): The TurtleBot 4 is equipped with a camera to detect traffic lights using a YOLOv5 object detection model. The system classifies lights into three colors: red, green, and blue.</p> <p>Preprocessing: Brightness and contrast adjustments improve detection under varied lighting.</p> <p>Usage: Based on the traffic light color, the TurtleBot modifies its slowest speed at red, normal speed at green, and half speed at blue. In the demonstration, the robot accurately responds to these signals along its path.</p> <p>IMU (Inertial Measurement Unit): The IMU measures acceleration and rotational motion, allowing for accurate pose estimation.</p> <p>Filtering: Complementary filtering reduces noise for smooth orientation tracking.</p> <p>Usage: This data helps the TurtleBot maintain stability and direction, especially during turns and transitions shown in the video.</p> <p>LiDAR (Light Detection and Ranging): The LiDAR sensor continuously scans the environment to detect obstacles and generate a local map.</p> <p>Data Conditioning: Filtering removes spurious data points to ensure reliability.</p> <p>Usage: The TurtleBot uses this data for obstacle avoidance and safe route planning in a dynamic environment.</p> <p>Sensor Fusion: Sensor data from the camera, IMU, and LiDAR is combined to support adaptive navigation and interaction.</p> <p>Usage: The TurtleBot can recognize visual cues (traffic lights) while simultaneously avoiding obstacles and maintaining smooth motion, as seen in the integrated environment.</p> <p>UR5 Robotic Arm Integration: The UR5 robot arm is incorporated to represent the project\u2019s end goal of multi-robot coordination.</p> <p>Communication: Though limited by technical constraints during testing, the system is designed for ROS2-based communication between TurtleBot 4 and UR5.</p> <p>Usage: In the final demonstration, the UR5 shows simulated arm movement as a placeholder for the intended object-handling or sorting task. The video illustrates how the two robots are expected to collaborate\u2014TurtleBot for transport and detection, UR5 for manipulation.</p> <p>In summary, this system demonstrates an intelligent multi-robot collaboration where autonomous navigation, traffic light response, and robotic manipulation can work together, with sensor fusion ensuring robust real-time performance in semi-structured environments.</p>"},{"location":"#demo-video","title":"DEMO Video","text":"<p>This video shows a function that can identify which color has the largest area.(03/11/2025)    </p> <p>This video shows a function where the car follows a red object and stops when it gets close enough.(03/11/2025)    </p> <p>This video shows the TurtleBot following commands such as start, stop, and emergency stop. Simultaneously, another window displays the line chart from the TurtleBot's acceleration data. (03/31/2025)    </p> <p>This video shows the TurtleBot following commands such as start, stop, and emergency stop. Simultaneously, another window displays the line chart from the TurtleBot's acceleration data. (04/14/2025)    </p>"},{"location":"#final-demonstration05012025","title":"Final Demonstration(05/01/2025)","text":"<p>Due to the large number of devices on the classroom network, the signal was unstable, causing frequent disconnections between the virtual machine and the UR robot arm.</p> <p>Additionally, when the virtual machine was connected to the UR while also running ROS2 on the TurtleBot, the system would publish an excessive number of topics, resulting in frequent communication failures. Therefore, the actions shown in the final video serve as a conceptual demonstration, representing the intended goal of our project rather than a fully synchronized real-time execution.</p>"},{"location":"#final-simulation-video05012025","title":"Final Simulation Video(05/01/2025)","text":""},{"location":"#final-demonstration-video05012025","title":"Final Demonstration Video(05/01/2025)","text":""},{"location":"#summary","title":"Summary","text":"<p>There are several potential improvements for this project. First, we could enhance the stability of the UR5\u2019s connection to the virtual machine by installing a dual-boot system on the computer, eliminating the need for a bridge adapter and ensuring a more secure and consistent connection. Additionally, to better synchronize all ROS 2 topics, implementing a single RCLPY spinner could help maintain a manageable computational load on the virtual machine. Lastly, offloading some or all of the computing tasks\u2014such as color recognition\u2014to the TurtleBot itself could significantly improve its response time, resulting in smoother operation.</p> <p>Overall, this project provided valuable lessons in integrating and coordinating multiple devices using ROS 2. We gained hands-on experience with ROS 2 communication and developed a foundational understanding of Linux systems, including configuring networks for multi-robot setups. These skills are directly applicable to real-world factory environments and will undoubtedly support our future careers in robotics and automation.</p>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\nloader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\nconst svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\nconsole.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"}]}